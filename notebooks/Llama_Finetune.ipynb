{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34dc377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhoom\\OneDrive\\Data Engineering Interview kit\\BiasGuard\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.5 | OS: Windows\n",
      "Transformers: 4.57.1 | datasets: 4.3.0\n",
      "PyTorch: 2.9.0+cpu | CUDA: False\n"
     ]
    }
   ],
   "source": [
    "import sys, platform\n",
    "import transformers, datasets, torch\n",
    "print(\"Python:\", sys.version.split()[0], \"| OS:\", platform.system())\n",
    "print(\"Transformers:\", transformers.__version__, \"| datasets:\", datasets.__version__)\n",
    "print(\"PyTorch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7cf8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"sshleifer/tiny-gpt2\"   # swap later to a Llama checkpoint if you have GPU\n",
    "BLOCK_SIZE = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b85c564b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhoom\\OneDrive\\Data Engineering Interview kit\\BiasGuard\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bhoom\\.cache\\huggingface\\hub\\datasets--wikitext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 46313.84 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 917989.18 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 362542.14 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 200, {'text': ''})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# small raw text dataset; fast to download\n",
    "raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "# keep very small slices to make CPU training fast\n",
    "train_raw = raw[\"train\"].select(range(1000))\n",
    "valid_raw = raw[\"validation\"].select(range(200))\n",
    "\n",
    "len(train_raw), len(valid_raw), train_raw[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6f93fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhoom\\OneDrive\\Data Engineering Interview kit\\BiasGuard\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bhoom\\.cache\\huggingface\\hub\\models--sshleifer--tiny-gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 5100.22 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 6006.58 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2686.63 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 2639.70 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(482,\n",
       " 104,\n",
       " {'input_ids': [796,\n",
       "   569,\n",
       "   18354,\n",
       "   7496,\n",
       "   17740,\n",
       "   6711,\n",
       "   796,\n",
       "   220,\n",
       "   198,\n",
       "   2311,\n",
       "   73,\n",
       "   13090,\n",
       "   645,\n",
       "   569,\n",
       "   18354,\n",
       "   7496,\n",
       "   513,\n",
       "   1058,\n",
       "   791,\n",
       "   47398,\n",
       "   17740,\n",
       "   357,\n",
       "   4960,\n",
       "   1058,\n",
       "   10545,\n",
       "   230,\n",
       "   99,\n",
       "   161,\n",
       "   254,\n",
       "   112,\n",
       "   5641,\n",
       "   44444,\n",
       "   9202,\n",
       "   25084,\n",
       "   24440,\n",
       "   12675,\n",
       "   11839,\n",
       "   18,\n",
       "   837,\n",
       "   6578,\n",
       "   764,\n",
       "   569,\n",
       "   18354,\n",
       "   7496,\n",
       "   286,\n",
       "   262,\n",
       "   30193,\n",
       "   513,\n",
       "   1267,\n",
       "   837,\n",
       "   8811,\n",
       "   6412,\n",
       "   284,\n",
       "   355,\n",
       "   569,\n",
       "   18354,\n",
       "   7496,\n",
       "   17740,\n",
       "   6711,\n",
       "   2354,\n",
       "   2869,\n",
       "   837,\n",
       "   318,\n",
       "   257,\n",
       "   16106,\n",
       "   2597,\n",
       "   2488,\n",
       "   12,\n",
       "   31,\n",
       "   2712,\n",
       "   2008,\n",
       "   983,\n",
       "   4166,\n",
       "   416,\n",
       "   29490,\n",
       "   290,\n",
       "   6343,\n",
       "   13,\n",
       "   44206,\n",
       "   329,\n",
       "   262,\n",
       "   14047,\n",
       "   44685,\n",
       "   764,\n",
       "   28728,\n",
       "   287,\n",
       "   3269,\n",
       "   2813,\n",
       "   287,\n",
       "   2869,\n",
       "   837,\n",
       "   340,\n",
       "   318,\n",
       "   262,\n",
       "   2368,\n",
       "   983,\n",
       "   287,\n",
       "   262,\n",
       "   569,\n",
       "   18354,\n",
       "   7496,\n",
       "   2168,\n",
       "   764,\n",
       "   12645,\n",
       "   278,\n",
       "   262,\n",
       "   976,\n",
       "   21748,\n",
       "   286,\n",
       "   16106,\n",
       "   290,\n",
       "   1103,\n",
       "   2488,\n",
       "   12,\n",
       "   31,\n",
       "   640,\n",
       "   11327,\n",
       "   355,\n",
       "   663,\n",
       "   27677,\n",
       "   837,\n",
       "   262,\n",
       "   1621,\n",
       "   4539,\n",
       "   10730,\n",
       "   284,\n",
       "   262,\n",
       "   717],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  'labels': [796,\n",
       "   569,\n",
       "   18354,\n",
       "   7496,\n",
       "   17740,\n",
       "   6711,\n",
       "   796,\n",
       "   220,\n",
       "   198,\n",
       "   2311,\n",
       "   73,\n",
       "   13090,\n",
       "   645,\n",
       "   569,\n",
       "   18354,\n",
       "   7496,\n",
       "   513,\n",
       "   1058,\n",
       "   791,\n",
       "   47398,\n",
       "   17740,\n",
       "   357,\n",
       "   4960,\n",
       "   1058,\n",
       "   10545,\n",
       "   230,\n",
       "   99,\n",
       "   161,\n",
       "   254,\n",
       "   112,\n",
       "   5641,\n",
       "   44444,\n",
       "   9202,\n",
       "   25084,\n",
       "   24440,\n",
       "   12675,\n",
       "   11839,\n",
       "   18,\n",
       "   837,\n",
       "   6578,\n",
       "   764,\n",
       "   569,\n",
       "   18354,\n",
       "   7496,\n",
       "   286,\n",
       "   262,\n",
       "   30193,\n",
       "   513,\n",
       "   1267,\n",
       "   837,\n",
       "   8811,\n",
       "   6412,\n",
       "   284,\n",
       "   355,\n",
       "   569,\n",
       "   18354,\n",
       "   7496,\n",
       "   17740,\n",
       "   6711,\n",
       "   2354,\n",
       "   2869,\n",
       "   837,\n",
       "   318,\n",
       "   257,\n",
       "   16106,\n",
       "   2597,\n",
       "   2488,\n",
       "   12,\n",
       "   31,\n",
       "   2712,\n",
       "   2008,\n",
       "   983,\n",
       "   4166,\n",
       "   416,\n",
       "   29490,\n",
       "   290,\n",
       "   6343,\n",
       "   13,\n",
       "   44206,\n",
       "   329,\n",
       "   262,\n",
       "   14047,\n",
       "   44685,\n",
       "   764,\n",
       "   28728,\n",
       "   287,\n",
       "   3269,\n",
       "   2813,\n",
       "   287,\n",
       "   2869,\n",
       "   837,\n",
       "   340,\n",
       "   318,\n",
       "   262,\n",
       "   2368,\n",
       "   983,\n",
       "   287,\n",
       "   262,\n",
       "   569,\n",
       "   18354,\n",
       "   7496,\n",
       "   2168,\n",
       "   764,\n",
       "   12645,\n",
       "   278,\n",
       "   262,\n",
       "   976,\n",
       "   21748,\n",
       "   286,\n",
       "   16106,\n",
       "   290,\n",
       "   1103,\n",
       "   2488,\n",
       "   12,\n",
       "   31,\n",
       "   640,\n",
       "   11327,\n",
       "   355,\n",
       "   663,\n",
       "   27677,\n",
       "   837,\n",
       "   262,\n",
       "   1621,\n",
       "   4539,\n",
       "   10730,\n",
       "   284,\n",
       "   262,\n",
       "   717]})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tok(examples[\"text\"], return_special_tokens_mask=False)\n",
    "\n",
    "train_tok = train_raw.map(tokenize, batched=True, remove_columns=train_raw.column_names)\n",
    "valid_tok = valid_raw.map(tokenize, batched=True, remove_columns=valid_raw.column_names)\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate then split into fixed-size blocks\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_len = (len(concatenated[\"input_ids\"]) // BLOCK_SIZE) * BLOCK_SIZE\n",
    "    result = {\n",
    "        k: [t[:total_len] for t in [concatenated[k]]][0] for k in concatenated.keys()\n",
    "    }\n",
    "    result = {\n",
    "        k: [result[k][i : i + BLOCK_SIZE] for i in range(0, total_len, BLOCK_SIZE)]\n",
    "        for k in result.keys()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "train_blocks = train_tok.map(group_texts, batched=True)\n",
    "valid_blocks = valid_tok.map(group_texts, batched=True)\n",
    "\n",
    "len(train_blocks), len(valid_blocks), train_blocks[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c511f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de2f9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GPT2LMHeadModel'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.__class__.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae1715da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhoom\\AppData\\Local\\Temp\\ipykernel_26360\\1622694960.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
      "c:\\Users\\bhoom\\OneDrive\\Data Engineering Interview kit\\BiasGuard\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61' max='61' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [61/61 00:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.733300</td>\n",
       "      <td>10.726302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 10.726302146911621,\n",
       " 'eval_runtime': 0.6959,\n",
       " 'eval_samples_per_second': 149.437,\n",
       " 'eval_steps_per_second': 18.68,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"outputs/llama_like_tiny_causal_lm\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_blocks,\n",
    "    eval_dataset=valid_blocks,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tok,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_res = trainer.evaluate()\n",
    "eval_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a4cdb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The quick brown fox stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tok, device=-1)\n",
    "out = pipe(\"The quick brown fox\", max_new_tokens=30, do_sample=False)\n",
    "out[0][\"generated_text\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
